"""
åˆ¸å•†ç±Œç¢¼é¸è‚¡ç¨‹å¼
"""

import requests
from lxml import html
import pandas as pd
import re
import time
import os


class BrokerCrawler:
    """åˆ¸å•†è³‡æ–™çˆ¬èŸ²"""
    
    def __init__(self, url):
        self.url = url
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.7',
        }
        self.tree = None
        self.html_content = None
    
    def fetch_page(self):
        """æŠ“å–ç¶²é """
        try:
            response = requests.get(self.url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # å˜—è©¦ä¸åŒç·¨ç¢¼
            for encoding in ['big5', 'cp950', 'gb2312', 'utf-8']:
                try:
                    self.html_content = response.content.decode(encoding)
                    return True
                except UnicodeDecodeError:
                    continue
            
            # æœ€å¾Œå˜—è©¦å¿½ç•¥éŒ¯èª¤
            self.html_content = response.content.decode('big5', errors='ignore')
            return True
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•—: {e}")
            return False
    
    def parse_html(self):
        """è§£æ HTML"""
        if not self.html_content:
            return False
        try:
            self.tree = html.fromstring(self.html_content)
            return True
        except:
            return False
    
    def extract_stock_id(self, script_text):
        """æå–è‚¡ç¥¨ä»£è™Ÿ"""
        match = re.search(r"GenLink2stk\('AS(\d+)'", script_text)
        return match.group(1) if match else None
    
    def extract_stock_name(self, script_text):
        """æå–è‚¡ç¥¨åç¨±"""
        match = re.search(r"GenLink2stk\('AS\d+','([^']+)'", script_text)
        return match.group(1) if match else None
    
    def parse_number(self, text):
        """è½‰æ›æ•¸å­—"""
        if not text:
            return 0
        clean_text = text.replace(',', '').replace(' ', '').replace('%', '').strip()
        try:
            return int(clean_text)
        except ValueError:
            return 0
    
    def crawl_chip_data(self, action):
        """æŠ“å–è²·è¶…æˆ–è³£è¶…è³‡æ–™"""
        if self.tree is None:
        #if not self.tree:
            return None
        
        headers = self.tree.xpath(f"//td[@class='t2' and text()='{action}']")
        if not headers:
            return None
        
        all_data = []
        for header in headers:
            parent_table = header.xpath("ancestor::table[@class='t0'][1]")[0]
            data_rows = parent_table.xpath(".//tr[position() > 2]")
            
            for row in data_rows:
                try:
                    script_elements = row.xpath(".//script")
                    if not script_elements:
                        continue
                    
                    script_text = script_elements[0].text_content()
                    stock_id = self.extract_stock_id(script_text)
                    stock_name = self.extract_stock_name(script_text)
                    number_cells = row.xpath(".//td[@class='t3n1']")
                    
                    if len(number_cells) >= 3:
                        diff_amount = self.parse_number(number_cells[2].text_content())
                        all_data.append({
                            'stock_id': stock_id,
                            'stock_name': stock_name,
                            'diff_amount': diff_amount
                        })
                except:
                    continue
        
        return all_data
    
    def crawl_stock_detail(self):
        """
        æŠ“å–å€‹è‚¡åˆ¸å•†é€²å‡ºæ˜ç´°ï¼ˆæ­£ç¢ºç‰ˆï¼‰
        çµæ§‹ï¼šè²·è¶…å’Œè³£è¶…åœ¨åŒä¸€å€‹ TR ä¸­
        
        HTML çµæ§‹ï¼š
        <TR>
            <TD>è²·è¶…åˆ¸å•†åç¨±</TD>      <- cells[0]
            <TD>è²·é€²</TD>             <- cells[1]
            <TD>è³£å‡º</TD>             <- cells[2]
            <TD>è²·è¶…</TD>             <- cells[3]
            <TD>ä½”æˆäº¤æ¯”é‡</TD>         <- cells[4]
            <TD>è³£è¶…åˆ¸å•†åç¨±</TD>      <- cells[5]
            <TD>è²·é€²</TD>             <- cells[6]
            <TD>è³£å‡º</TD>             <- cells[7]
            <TD>è³£è¶…</TD>             <- cells[8]
            <TD>ä½”æˆäº¤æ¯”é‡</TD>         <- cells[9]
        </TR>
        
        è¿”å›: {'buy_top5': [...], 'sell_top5': [...]}
        """
        if self.tree is None:
        #if not self.tree:
            return None
        
        result = {'buy_top5': [], 'sell_top5': []}
        
        # æ‰¾åˆ°åŒ…å«ã€Œè²·è¶…åˆ¸å•†ã€å’Œã€Œè³£è¶…åˆ¸å•†ã€æ¨™é¡Œçš„ TR
        header_row = self.tree.xpath("//td[@class='t2' and text()='è²·è¶…åˆ¸å•†']")
        
        if not header_row:
            return result
        
        # æ‰¾åˆ°æ¨™é¡Œæ‰€åœ¨çš„ TRï¼Œç„¶å¾Œæ‰¾å¾ŒçºŒçš„å…„å¼Ÿ TRï¼ˆè³‡æ–™åˆ—ï¼‰
        parent_tr = header_row[0].xpath("ancestor::tr[1]")[0]
        
        # æ‰¾åˆ°æ‰€æœ‰å¾ŒçºŒçš„è³‡æ–™åˆ—
        data_rows = parent_tr.xpath("following-sibling::tr")
        
        buy_list = []
        sell_list = []
        
        for row in data_rows:
            cells = row.xpath(".//td")
            
            # è‡³å°‘éœ€è¦ 10 å€‹æ¬„ä½ï¼ˆè²·è¶…5å€‹ + è³£è¶…5å€‹ï¼‰
            if len(cells) < 10:
                continue
            
            try:
                # è§£æè²·è¶…è³‡æ–™ï¼ˆå‰5å€‹æ¬„ä½ï¼‰
                buy_broker_elem = cells[0].xpath(".//a")
                if buy_broker_elem:
                    buy_broker_name = buy_broker_elem[0].text_content().strip()
                    buy_amount = self.parse_number(cells[1].text_content())
                    sell_amount = self.parse_number(cells[2].text_content())
                    diff_amount = self.parse_number(cells[3].text_content())
                    
                    # åªè¨˜éŒ„æœ‰æ•ˆçš„è²·è¶…è³‡æ–™
                    if buy_broker_name and diff_amount != 0:
                        buy_list.append({
                            'broker_name': buy_broker_name,
                            'buy': buy_amount,
                            'sell': sell_amount,
                            'diff': diff_amount
                        })
            except:
                pass
            
            try:
                # è§£æè³£è¶…è³‡æ–™ï¼ˆå¾Œ5å€‹æ¬„ä½ï¼‰
                sell_broker_elem = cells[5].xpath(".//a")
                if sell_broker_elem:
                    sell_broker_name = sell_broker_elem[0].text_content().strip()
                    buy_amount = self.parse_number(cells[6].text_content())
                    sell_amount = self.parse_number(cells[7].text_content())
                    diff_amount = self.parse_number(cells[8].text_content())
                    
                    # è³£è¶…çš„å·®é¡æ‡‰è©²æ˜¯è² æ•¸ï¼Œä½†ç¶²é ä¸Šå¯èƒ½åªé¡¯ç¤ºçµ•å°å€¼
                    # ç¢ºä¿è³£è¶…æ˜¯è² æ•¸
                    if diff_amount > 0:
                        diff_amount = -diff_amount
                    
                    # åªè¨˜éŒ„æœ‰æ•ˆçš„è³£è¶…è³‡æ–™
                    if sell_broker_name and diff_amount != 0:
                        sell_list.append({
                            'broker_name': sell_broker_name,
                            'buy': buy_amount,
                            'sell': sell_amount,
                            'diff': diff_amount
                        })
            except:
                pass
        
        # å–å‰5å
        result['buy_top5'] = buy_list[:5]
        result['sell_top5'] = sell_list[:5]
        
        return result


def download_broker_chips(trans_date, output_dir='data'):
    """æ­¥é©Ÿ 1: ä¸‹è¼‰å„åˆ¸å•†è²·è³£è¶…è³‡æ–™"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ“¥ æ­¥é©Ÿ 1: ä¸‹è¼‰åˆ¸å•†è²·è³£è¶…è³‡æ–™")
    print(f"{'='*70}")
    
    # æ—¥æœŸæ ¼å¼è½‰æ›
    year, month, day = trans_date.split('-')
    formatted_date = f"{year}-{int(month)}-{int(day)}"
    
    # å¤–è³‡åˆ¸å•†æ¸…å–®
    brokers = {
        "å°ç£æ‘©æ ¹å£«ä¸¹åˆ©": 1470, 
        "æ‘©æ ¹å¤§é€š": 8440,
        "ç¾å•†é«˜ç››": 1480, 
        "ç¾æ—": 1440,
        "èŠ±æ——ç’°çƒ": 1590,
        "æ³•éŠ€å·´é»": 8900,
        "æ–°åŠ å¡å•†ç‘éŠ€": 1650,
        "é¦™æ¸¯ä¸Šæµ·åŒ¯è±": 8960
    }
    
    all_broker_data = {}
    
    # é€å€‹åˆ¸å•†æŠ“å–
    for broker_name, broker_id in brokers.items():
        print(f"  ğŸ“Š {broker_name}...", end=' ')
        
        url = f"https://fubon-ebrokerdj.fbs.com.tw/z/zg/zgb/zgb0.djhtm?a={broker_id}&b={broker_id}&c=B&e={formatted_date}&f={formatted_date}"
        crawler = BrokerCrawler(url)
        
        if not crawler.fetch_page() or not crawler.parse_html():
            print("âŒ")
            time.sleep(1)
            continue
        
        buy_data = crawler.crawl_chip_data("è²·è¶…")
        sell_data = crawler.crawl_chip_data("è³£è¶…")
        
        if buy_data or sell_data:
            combined_data = []
            if buy_data:
                for item in buy_data:
                    item['type'] = 'è²·è¶…'
                    combined_data.append(item)
            if sell_data:
                for item in sell_data:
                    item['type'] = 'è³£è¶…'
                    combined_data.append(item)
            
            all_broker_data[broker_name] = pd.DataFrame(combined_data)
            print(f"âœ… ({len(combined_data)} ç­†)")
        else:
            print("âš ï¸")
        
        time.sleep(2)
    
    # å„²å­˜ Excel
    if all_broker_data:
        output_file = f"{output_dir}/chip_{trans_date}.xlsx"
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            all_data = []
            for broker_name, df in all_broker_data.items():
                df_copy = df.copy()
                df_copy['broker'] = broker_name
                all_data.append(df_copy)
            
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df = combined_df[['broker', 'type', 'stock_id', 'stock_name', 'diff_amount']]
            combined_df.columns = ['åˆ¸å•†', 'é¡å‹', 'è‚¡ç¥¨ä»£è™Ÿ', 'è‚¡ç¥¨åç¨±', 'è²·è³£è¶…å¼µæ•¸']
            combined_df.to_excel(writer, sheet_name='å…¨éƒ¨è³‡æ–™', index=False)
        
        print(f"\nâœ… å·²å„²å­˜: {output_file}")
        print(f"ğŸ“Š ç¸½è³‡æ–™: {len(combined_df)} ç­†")
        return output_file
    
    return None


def filter_strong_buy_stocks(chip_file):
    """æ­¥é©Ÿ 2: ç¯©é¸ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ” æ­¥é©Ÿ 2: ç¯©é¸ ã€Œè‡³å°‘ 3 å®¶å¤–è³‡è²·è¶…ã€ çš„è‚¡ç¥¨")
    print(f"{'='*70}")
    
    df = pd.read_excel(chip_file, sheet_name='å…¨éƒ¨è³‡æ–™')
    
    # å»ºç«‹è³£è¶…æ± 
    sell_pool = set(df[df["é¡å‹"] == "è³£è¶…"]["è‚¡ç¥¨åç¨±"])
    print(f"  ğŸš« è³£è¶…è‚¡ç¥¨æ•¸: {len(sell_pool)}")
    
    # ç¯©é¸ï¼šè²·è¶…æ¬¡æ•¸ > 3 ä¸”ä¸åœ¨è³£è¶…æ± 
    buy_df = df[df["é¡å‹"] == "è²·è¶…"]
    buy_result = buy_df.groupby("è‚¡ç¥¨åç¨±")\
                 .filter(lambda x: len(x) >= 3 and x.name not in sell_pool)["è‚¡ç¥¨ä»£è™Ÿ"]\
                 .drop_duplicates()\
                 .tolist()
    
    print(f"  âœ… ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨: {len(buy_result)} æª”")
    if buy_result:
        print(f"     {', '.join(str(s) for s in buy_result[:10])}{'...' if len(buy_result) > 10 else ''}")
    
    
    tmp = [df[df["è‚¡ç¥¨ä»£è™Ÿ"]==stock_id]["è‚¡ç¥¨åç¨±"].iloc[0] for stock_id in buy_result]
    buy_result_v2 = {"è‚¡ç¥¨ä»£è™Ÿ": buy_result,
                     "è‚¡ç¥¨åç¨±": tmp}
    return buy_result_v2


def crawl_stock_details(stock_dict, trans_date):
    """æ­¥é©Ÿ 3: æŠ“å–å€‹è‚¡åˆ¸å•†é€²å‡ºæ˜ç´°"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ“¥ æ­¥é©Ÿ 3: æŠ“å–å€‹è‚¡åˆ¸å•†é€²å‡ºæ˜ç´°")
    print(f"{'='*70}")
    
    year, month, day = trans_date.split('-')
    formatted_date = f"{year}-{int(month)}-{int(day)}"
    
    # å¤–è³‡åˆ¸å•†åç¨±ï¼ˆç”¨æ–¼åˆ¤æ–·ï¼‰
    foreign_brokers = {
        "å°ç£æ‘©æ ¹å£«ä¸¹åˆ©", "æ‘©æ ¹å¤§é€š", "ç¾å•†é«˜ç››", "ç¾æ—", 
        "èŠ±æ——ç’°çƒ", "æ³•éŠ€å·´é»", "æ–°åŠ å¡å•†ç‘éŠ€", "é¦™æ¸¯ä¸Šæµ·åŒ¯è±"
    }
    
    stock_details = []
    
    stock_list = stock_dict["è‚¡ç¥¨ä»£è™Ÿ"]
    stock_names = stock_dict["è‚¡ç¥¨åç¨±"]
    for idx, stock_id in enumerate(stock_list, 1):
        # å–å¾—è‚¡ç¥¨åç¨±
        stock_name = stock_names[idx-1]
        # stock_name_elem = crawler.tree.xpath("//span[@class='t3n1']")
        # stock_name = stock_name_elem[0].text_content().strip() if stock_name_elem else stock_id
        print(f"  [{idx}/{len(stock_list)}] {stock_id} {stock_name} ...", end=' ')
        
        url = f"https://fubon-ebrokerdj.fbs.com.tw/z/zc/zco/zco.djhtm?a={stock_id}&e={formatted_date}&f={formatted_date}"
        crawler = BrokerCrawler(url)
        
        if not crawler.fetch_page() or not crawler.parse_html():
            print("âŒ")
            time.sleep(1)
            continue
        
        detail = crawler.crawl_stock_detail()
        
        if not detail:
            print("âš ï¸ (ç„¡è³‡æ–™)")
            time.sleep(1)
            continue
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è³£è¶…è³‡æ–™
        if not detail['sell_top5']:
            print("âœ… (ç„¡è³£è¶…)")
            # æ²’æœ‰è³£è¶…ä¹Ÿç®—ç¬¦åˆæ¢ä»¶
        else:
            # æª¢æŸ¥è³£è¶…å‰äº”å¤§æ˜¯å¦æœ‰å¤–è³‡
            sell_top5_brokers = [b['broker_name'] for b in detail['sell_top5']]
            has_foreign = any(
                any(foreign in broker for foreign in foreign_brokers)
                for broker in sell_top5_brokers
            )
            
            if has_foreign:
                print("ğŸš« (æœ‰å¤–è³‡è³£è¶…)")
                time.sleep(1)
                continue
        
        
        # æ•´ç†è²·è¶…å’Œè³£è¶…å‰äº”å¤§è³‡æ–™
        buy_top5_names = [b['broker_name'] for b in detail.get('buy_top5', [])]
        buy_top5_amounts = [str(b['diff']) for b in detail.get('buy_top5', [])]
        sell_top5_names = [b['broker_name'] for b in detail.get('sell_top5', [])]
        sell_top5_amounts = [str(abs(b['diff'])) for b in detail.get('sell_top5', [])]  # å–çµ•å°å€¼
        
        stock_details.append({
            'stock_id': stock_id,
            'stock_name': stock_name,
            'buy_top5_names': ','.join(buy_top5_names) if buy_top5_names else '-',
            'buy_top5_amounts': ','.join(buy_top5_amounts) if buy_top5_amounts else '-',
            'sell_top5_names': ','.join(sell_top5_names) if sell_top5_names else '-',
            'sell_top5_amounts': ','.join(sell_top5_amounts) if sell_top5_amounts else '-'
        })
        
        print("âœ…")
        time.sleep(2)
    
    print(f"\nâœ… æœ€çµ‚ç¯©é¸å‡º: {len(stock_details)} æª”è‚¡ç¥¨")
    
    return stock_details


def save_filtered_results(stock_details, trans_date, output_dir='data'):
    """æ­¥é©Ÿ 4: å„²å­˜æœ€çµ‚çµæœ"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ æ­¥é©Ÿ 4: å„²å­˜ç¯©é¸çµæœ")
    print(f"{'='*70}")
    
    if not stock_details:
        print("  âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
        return None
    
    df = pd.DataFrame(stock_details)
    df.columns = ['è‚¡ç¥¨ä»£è™Ÿ', 'è‚¡ç¥¨åç¨±', 'è²·è¶…å‰äº”åˆ¸å•†', 'è²·è¶…å¼µæ•¸', 'è³£è¶…å‰äº”åˆ¸å•†', 'è³£è¶…å¼µæ•¸']
    
    output_file = f"{output_dir}/chip_filtered_{trans_date}.xlsx"
    
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='ç¯©é¸çµæœ', index=False)
    
    print(f"  âœ… å·²å„²å­˜: {output_file}")
    print(f"  ğŸ“Š å…± {len(df)} æª”è‚¡ç¥¨")
    print(f"\n{'='*70}")
    print("å‰ 5 æª”é è¦½:")
    print(f"{'='*70}")
    print(df.head().to_string(index=False))
    
    return output_file


def main():
    """ä¸»ç¨‹å¼"""
    
    # ============ è¨­å®šå€ ============
    trans_date = "2026-01-16"  # äº¤æ˜“æ—¥æœŸ
    output_dir = "data"        # è¼¸å‡ºç›®éŒ„
    # ===============================
    
    os.makedirs(output_dir, exist_ok=True)
    
    print("\n" + "="*70)
    print("ğŸš€ åˆ¸å•†ç±Œç¢¼é¸è‚¡ç¨‹å¼ï¼šé–‹å§‹åŸ·è¡Œ")
    print("="*70)
    print(f"ğŸ“… äº¤æ˜“æ—¥æœŸ: {trans_date}")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    try:
        # æ­¥é©Ÿ 1: ä¸‹è¼‰åˆ¸å•†è²·è³£è¶…è³‡æ–™
        chip_file = download_broker_chips(trans_date, output_dir)
        if not chip_file:
            print("\nâŒ ä¸‹è¼‰å¤±æ•—")
            return
        
        # æ­¥é©Ÿ 2: ç¯©é¸å¼·åŠ›è²·è¶…è‚¡ç¥¨
        stock_dict = filter_strong_buy_stocks(chip_file)
        if not stock_dict:
            print("\nâš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
            return
        
        # æ­¥é©Ÿ 3: æŠ“å–å€‹è‚¡æ˜ç´°ä¸¦éæ¿¾
        stock_details = crawl_stock_details(stock_dict, trans_date)
        
        # æ­¥é©Ÿ 4: å„²å­˜çµæœ
        output_file = save_filtered_results(stock_details, trans_date, output_dir)
        
        if output_file:
            print(f"\n{'='*70}")
            print("âœ… å…¨éƒ¨å®Œæˆï¼")
            print(f"{'='*70}")
            print(f"ğŸ“ æœ€çµ‚çµæœ: {output_file}")
        
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()